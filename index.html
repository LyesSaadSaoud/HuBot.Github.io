<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HuBot: A Biomimicking Mobile Robot for Non-Disruptive Bird Behavior Study and Ecological Conservation</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f4;
        }
        h1, h2, h3, h4 {
            color: #333;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        table, th, td {
            border: 1px solid #ddd;
        }
        th, td {
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
        img, video {
            display: block;
            margin: 20px auto;
            max-width: 100%;
        }
        .container {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
            gap: 0.02cm;
        }
        .container img {
            width: 15%; /* Adjust this percentage to change image size */
            margin-bottom: 0.02cm;
        }
        .caption {
            text-align: center;
            margin-top: 10px;
        }
    </style>
</head>
<body>

    <h1>HuBot: A Biomimicking Mobile Robot for Non-Disruptive Bird Behavior Study and Ecological Conservation</h1>
    <h2>Lyes Saad Saoud et al.</h2>
    <p>
      
     </p>
    <h2>Abstract</h2>
    <p>
        The Houbara bustard, a critically endangered avian species, poses significant challenges for researchers due to its elusive nature and sensitivity to human disturbance. Traditional research methods, often reliant on human observation, have yielded limited data and can inadvertently impact the bird’s behavior. To overcome these limitations, we propose for the first time HuBot, a biomimetic mobile robot designed to seamlessly integrate into the Houbara's natural habitat. Equipped with advanced real-time deep learning algorithms, including YOLOv9 for detection, MobileSAM for segmentation, and ViT for depth estimation, HuBot autonomously tracks individual birds, providing unprecedented insights into their movement patterns, social interactions, and habitat use. By accurately detecting and localizing the houbara, HuBot contributes to a deeper understanding of the Houbara's ecology and informs critical conservation decisions. The robot's biomimetic design, including life-like appearance and movement, minimizes disturbance, allowing for long-term, continuous monitoring without compromising data quality. Rigorous testing, including extensive laboratory experiments and captive trials with real Houbara birds, validated HuBot's performance and its potential to revolutionize the study of this enigmatic species. Through the deployment of HuBot, we aim to provide essential information for developing effective conservation strategies to safeguard the future of the Houbara bustard.
    </p>

    <!-- Placeholder for an image -->
    <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/images/HuBot_real.png" alt="HuBot in the field">

    <!-- Placeholder for a video -->
    <body>
      <div class="video-container">
          <video controls>
              <source src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/videos/book.mp4" type="video/mp4">
              Your browser does not support the video tag.
          </video>
          <video controls>
              <source src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/videos/HuBot.mp4" type="video/mp4">
              Your browser does not support the video tag.
          </video>
      </div>
  </body>
  

    <h2>Results</h2>
    <div class="container">
      <!-- Row 1 -->
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R11.jpg" alt="R11">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R12.jpg" alt="R12">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R13.jpg" alt="R13">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R14.jpg" alt="R14">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R15.jpg" alt="R15">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R16.jpg" alt="R16">
      <!-- Row 2 -->
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R21.jpg" alt="R21">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R22.jpg" alt="R22">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R23.jpg" alt="R23">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R24.jpg" alt="R24">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R25.jpg" alt="R25">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R26.jpg" alt="R26">
      <!-- Row 3 -->
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R31.jpg" alt="R31">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R32.jpg" alt="R32">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R33.jpg" alt="R33">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R34.jpg" alt="R34">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R35.jpg" alt="R35">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R36.jpg" alt="R36">
      <!-- Row 4 -->
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R41.jpg" alt="R41">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R42.jpg" alt="R42">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R43.jpg" alt="R43">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R44.jpg" alt="R44">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R45.jpg" alt="R45">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R46.jpg" alt="R46">
      <!-- Row 5 -->
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R51.jpg" alt="R51">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R52.jpg" alt="R52">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R53.jpg" alt="R53">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R54.jpg" alt="R54">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R55.jpg" alt="R55">
      <img src="C:/Users/saads/Downloads/nerfies.github.io-main/nerfies.github.io-main/static/figures/R56.jpg" alt="R56">
  </div>
  <div class="caption">
      <p>HuBot Performance in Real-World Houbara Interaction. This figure showcases the real-world capabilities of HuBot during an encounter with a Houbara bustard. Each column displays:</p>
      <p>(a) Input Image: The original image captured during the interaction.</p>
      <p>(b) Houbara Detection: The result of the Houbara detection process, likely using a bounding box around the detected Houbara.</p>
      <p>(c) Houbara Segmentation: The segmentation mask highlighting the detected Houbara region.</p>
      <p>(d) Local Houbara Depth Estimation: The depth map focusing on the Houbara, estimated using local features.</p>
      <p>(e) Masked Local Houbara Depth Estimation: An alternative depth map potentially utilizing a mask for improved accuracy around the Houbara.</p>
      <p>(f) Houbara Visual Localization: The Houbara's location visualized within the image frame.</p>
  </div>

    <h3>Object Detection and Segmentation Models</h3>
    <p>
        In our Houbara Observation Study, we present a detailed analysis of the performance of object detection and segmentation models tailored for the detection and segmentation of Houbara bustards. Key findings are summarized through Tables and Figures, which we discuss below.
    </p>

    <h4>Object Detection and Segmentation Models</h4>
    <p>
        Table 1 presents a comparative analysis of various object detection models based on key performance metrics. The mean Average Precision (mAP) at different Intersection over Union (IoU) thresholds offers a comprehensive evaluation of model accuracy in detecting Houbara bustards within varying levels of bounding box overlap. YOLOv9 consistently outperforms other models, achieving exceptional mAP scores across all IoU thresholds. This superior performance underscores YOLOv9's ability to accurately localize objects, even in challenging scenarios with partial occlusions or object deformations.
    </p>
    <p>
        The model’s high precision, recall, and F1-score further solidify its dominance. A precision of 0.9833 indicates a low false positive rate, minimizing the number of incorrect detections. Conversely, a recall of 0.9455 suggests that YOLOv9 effectively identifies most ground truth objects, reducing the risk of missed detections. The resulting F1-score of 0.9640 represents a balanced measure of precision and recall, demonstrating YOLOv9's overall effectiveness.
    </p>
    <p>
        Furthermore, YOLOv9's inference time of 0.0568 seconds is significantly faster than competing models, making it a prime candidate for real-time applications. This speed advantage is crucial for tracking dynamic objects like Houbara bustards, where rapid detection and response are essential.
    </p>
    <p>
        While other models, such as YOLOv8 and Faster R-CNN, exhibit commendable performance, they fall short of YOLOv9 in terms of overall accuracy and speed. Models like MobileNetV3 and Faster R-CNN-MobileNetV3 prioritize speed over accuracy, making them suitable for resource-constrained environments but less ideal for applications demanding high precision.
    </p>
    <p>
        YOLOv9 consistently outperforms other models in detecting Houbara bustards, demonstrating superior accuracy, speed, and robustness. Its exceptional performance across various metrics, including precision, recall, and F1-score, positions it as the ideal choice for real-time applications in wildlife conservation.
    </p>

    <table>
        <caption>Performance Metrics of Object Detection Models</caption>
        <thead>
            <tr>
                <th>Model Name</th>
                <th>mAP50</th>
                <th>mAP75</th>
                <th>mAP95</th>
                <th>Precision</th>
                <th>Recall</th>
                <th>F1-score</th>
                <th>Time (Sec.)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>RetinaNet</td>
                <td>0.8534</td>
                <td>0.6541</td>
                <td>0.5631</td>
                <td>0.8410</td>
                <td>0.8410</td>
                <td>0.8410</td>
                <td>1.9515</td>
            </tr>
            <tr>
                <td>FCOS</td>
                <td>0.7033</td>
                <td>0.5111</td>
                <td>0.4357</td>
                <td>0.7697</td>
                <td>0.7697</td>
                <td>0.7697</td>
                <td>1.4801</td>
            </tr>
            <tr>
                <td>MobileNet_V3</td>
                <td>0.7386</td>
                <td>0.5102</td>
                <td>0.4568</td>
                <td>0.7075</td>
                <td>0.7075</td>
                <td>0.7075</td>
                <td>0.1731</td>
            </tr>
            <tr>
                <td>FasterRCNN-ResNet-50</td>
                <td>0.8567</td>
                <td>0.7859</td>
                <td>0.6247</td>
                <td>0.8428</td>
                <td>0.8428</td>
                <td>0.8428</td>
                <td>2.3628</td>
            </tr>
            <tr>
                <td>FasterRCNN-MobileNetV3</td>
                <td>0.6734</td>
                <td>0.4336</td>
                <td>0.3972</td>
                <td>0.6856</td>
                <td>0.6856</td>
                <td>0.6856</td>
                <td>0.1046</td>
            </tr>
            <tr>
                <td>YOLOv5</td>
                <td>0.9100</td>
                <td>0.6588</td>
                <td>0.5858</td>
                <td>0.9103</td>
                <td>0.8331</td>
                <td>0.8700</td>
                <td>0.0840</td>
            </tr>
            <tr>
                <td>YOLOv8</td>
                <td>0.9208</td>
                <td>0.7555</td>
                <td>0.6435</td>
                <td>0.9325</td>
                <td>0.8283</td>
                <td>0.8773</td>
                <td>0.0739</td>
            </tr>
            <tr>
                <td><strong>YOLOv9</strong></td>
                <td><strong>0.9856</strong></td>
                <td><strong>0.9103</strong></td>
                <td><strong>0.7651</strong></td>
                <td><strong>0.9833</strong></td>
                <td><strong>0.9455</strong></td>
                <td><strong>0.9640</strong></td>
                <td><strong>0.0568</strong></td>
            </tr>
        </tbody>
    </table>

    <h4>Segmentation Models</h4>
    <table>
        <caption>Performance Metrics of Segmentation Models</caption>
        <thead>
            <tr>
                <th>Model Name</th>
                <th>mPLA</th>
                <th>mIoU</th>
                <th>Time (Sec.)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Mask-RCNN</td>
                <td>0.7842</td>
                <td>0.3455</td>
                <td>0.8273</td>
            </tr>
            <tr>
                <td>SegFormer</td>
                <td>0.7155</td>
                <td>0.3540</td>
                <td>0.1075</td>
            </tr>
            <tr>
                <td>YOLACT</td>
                <td>0.8098</td>
                <td>0.3895</td>
                <td>0.1332</td>
            </tr>
            <tr>
                <td>FCN</td>
                <td>0.6853</td>
                <td>0.2987</td>
                <td>0.0726</td>
            </tr>
            <tr>
                <td>YOLOv8-seg</td>
                <td>0.8002</td>
                <td>0.3532</td>
                <td>0.0692</td>
            </tr>
            <tr>
                <td><strong>MobileSAM</strong></td>
                <td><strong>0.8764</strong></td>
                <td><strong>0.5673</strong></td>
                <td><strong>0.0541</strong></td>
            </tr>
        </tbody>
    </table>

    <h4>Comparison of YOLOv9 with state-of-the-art object detection models:</h4>
    <ul>
        <li>Performance comparison of YOLOv9 with state-of-the-art object detection models across key metrics (mAP50, mAP75, mAP95, precision, recall, F1-score, and inference time).</li>
        <li>YOLOv9 demonstrated superior accuracy, precision, recall, and inference speed, achieving a mAP50 of 98.56%, mAP75 of 91.03%, and mAP95 of 76.51%.</li>
        <li>YOLOv9's precision of 98.33%, recall of 94.55%, and F1-score of 96.40% indicate its robustness in accurately detecting and localizing objects.</li>
        <li>The inference time of 0.0568 seconds per image showcases YOLOv9's efficiency and suitability for real-time applications.</li>
        <li>Table 1 presents a detailed comparison of YOLOv9 and other state-of-the-art models across various performance metrics.</li>
    </ul>

    <h4>MobileSAM: Effective Segmentation for Houbara Detection</h4>
    <ul>
        <li>Performance comparison of segmentation models for accurate identification and localization of Houbara bustards.</li>
        <li>MobileSAM achieved a mean Pixel-Level Accuracy (mPLA) of 87.64%, mean Intersection over Union (mIoU) of 56.73%, and an inference time of 0.0541 seconds per image.</li>
        <li>The results highlight MobileSAM's superior accuracy and efficiency compared to other segmentation models (Table 2).</li>
    </ul>

</body>
</html>
